1
00:00:00,840 --> 00:00:02,870
So one issue with either the SVD or

2
00:00:02,870 --> 00:00:05,850
principal components analysis is miss, is
always missing values.

3
00:00:05,850 --> 00:00:08,460
So real data will typically have missing
values, and the

4
00:00:08,460 --> 00:00:10,490
problem is that if you try to run the SVD

5
00:00:10,490 --> 00:00:12,700
on a data set that doesn't have or that
has

6
00:00:12,700 --> 00:00:14,870
some missing values, like you see that
I've created here.

7
00:00:14,870 --> 00:00:16,860
You can see that you get an error.

8
00:00:16,860 --> 00:00:19,510
You just can't run it on a data set that
has missing values.

9
00:00:19,510 --> 00:00:22,470
So you need to do something about the
missing values.

10
00:00:22,470 --> 00:00:24,560
Before you run an SVD or a PCA.

11
00:00:26,160 --> 00:00:29,640
So, one possibility and there are many
others though, is the,

12
00:00:29,640 --> 00:00:33,740
use the impute package which is available
from the bioconductor project.

13
00:00:33,740 --> 00:00:38,330
and, and just impute the missing datas,
sorry the missing data points and so

14
00:00:38,330 --> 00:00:40,740
that you can have a value there, and then
you can run your SVD.

15
00:00:40,740 --> 00:00:42,210
And so this approach you.

16
00:00:42,210 --> 00:00:47,920
This code here uses the impute.knn
function which takes a, a missing row or

17
00:00:47,920 --> 00:00:51,460
missing values in a row, and imputes it by
the k nearest neighbors to

18
00:00:51,460 --> 00:00:52,790
that row.

19
00:00:52,790 --> 00:00:56,050
So if k, for example, is five, then it
will

20
00:00:56,050 --> 00:00:58,790
take the five rows that are closest to the
row with

21
00:00:58,790 --> 00:01:01,290
the missing data, and then impute the data
in that

22
00:01:01,290 --> 00:01:04,220
missing row with the kind of average of
the other five.

23
00:01:04,220 --> 00:01:06,780
And so, once we've imputed the data with

24
00:01:06,780 --> 00:01:10,140
this impute.knn function we can run the
svd.

25
00:01:10,140 --> 00:01:11,400
You can see, it runs without error.

26
00:01:11,400 --> 00:01:16,600
And we can, we can kind of plot the first
singular

27
00:01:16,600 --> 00:01:19,430
vectors from each of them.

28
00:01:19,430 --> 00:01:23,390
And so you can see that the on the left
hand side, I've got the,

29
00:01:23,390 --> 00:01:25,040
the data, the, kind of the, the

30
00:01:25,040 --> 00:01:28,010
first singular vector from the original
data matrix,

31
00:01:28,010 --> 00:01:29,570
and the second, on the right-hand side,

32
00:01:29,570 --> 00:01:31,320
I've got the, server singular vector from
the

33
00:01:31,320 --> 00:01:35,410
data matrix that was in, that was kind of
where the missing data was imputed.

34
00:01:35,410 --> 00:01:37,030
Now, you can see that they're roughly
similar.

35
00:01:37,030 --> 00:01:39,170
They're not exactly the same, but the
imputation

36
00:01:39,170 --> 00:01:42,150
didn't seem to have a major effect on

37
00:01:42,150 --> 00:01:44,610
the on the running of the svd.
So

38
00:01:46,840 --> 00:01:49,928
this is a final example here, it's just
kind of an interesting example.

39
00:01:49,928 --> 00:01:51,150
I just want to show how you can take an

40
00:01:51,150 --> 00:01:54,197
actual image, which is represented as a
matrix, and

41
00:01:54,197 --> 00:01:57,460
kind of and, and, and kind of develop a

42
00:01:57,460 --> 00:02:01,030
lower dimensional or lower rank
representation of this actual image.

43
00:02:01,030 --> 00:02:02,480
So here's a picture of a face.

44
00:02:02,480 --> 00:02:05,480
It's a relatively low resolution picture
of a face, but you can see that there

45
00:02:05,480 --> 00:02:08,440
is a, you know, a nose and ears and two
eyes and a mouth there.

46
00:02:08,440 --> 00:02:11,940
And so what we're going to do is we're
going to run the svd

47
00:02:11,940 --> 00:02:15,000
on this face data and look at the variance
explained.

48
00:02:15,000 --> 00:02:17,150
And so you can see that the first

49
00:02:17,150 --> 00:02:20,570
singular vector explains about 40% of the
variation.

50
00:02:20,570 --> 00:02:23,100
And then the second is about say 20 some
percent.

51
00:02:23,100 --> 00:02:25,796
And then the third one is about maybe 15%.

52
00:02:25,796 --> 00:02:30,870
And so if you look at say the first five
to ten singular

53
00:02:30,870 --> 00:02:35,070
vectors they capture pretty much all of
the variation in the data set.

54
00:02:35,070 --> 00:02:37,350
And so we can, we can see, we can actually

55
00:02:37,350 --> 00:02:40,100
look at the image that's generated by say
the first

56
00:02:40,100 --> 00:02:42,940
singular vector, or the first five, or the
first ten.

57
00:02:44,580 --> 00:02:47,410
So so let's, so we can take a look at
that,

58
00:02:47,410 --> 00:02:49,200
and so the idea that we're going to use a
little bit

59
00:02:49,200 --> 00:02:52,840
of matrix multiplication here to create an
approximations of the space

60
00:02:53,890 --> 00:02:58,250
that is, that uses fewer components than
the original data set too.

61
00:02:58,250 --> 00:02:59,790
So here I'm creating one that just uses

62
00:02:59,790 --> 00:03:03,380
the first principle component the first
singular vector.

63
00:03:03,380 --> 00:03:06,140
I'm using one that takes the first five
altogether.

64
00:03:06,140 --> 00:03:08,178
And then another one that takes the first
ten.

65
00:03:08,178 --> 00:03:12,000
And, and so we can take a look at what
this approximation looks like.

66
00:03:12,000 --> 00:03:13,760
So the first image here, which is all the
way

67
00:03:13,760 --> 00:03:16,960
on the left here, just uses a single
singular vector.

68
00:03:16,960 --> 00:03:20,580
And you can see that it's not a very good,
it's not a pretty picture so to speak.

69
00:03:20,580 --> 00:03:21,630
There's not really a face there.

70
00:03:21,630 --> 00:03:23,050
There's not much you can see.

71
00:03:23,050 --> 00:03:29,610
But it's asking a lot to represent an
entire image using just a single vector.

72
00:03:29,610 --> 00:03:32,010
So, if we move on to the second one from
the left, you

73
00:03:32,010 --> 00:03:36,840
can see that basically most of the, of the
key features are already there.

74
00:03:36,840 --> 00:03:39,190
So this uses the first five singular
vectors.

75
00:03:39,190 --> 00:03:41,130
And you can see that clearly there's a
face.

76
00:03:41,130 --> 00:03:43,250
There's two eyes, a nose, and a mouth, and
two ears.

77
00:03:44,580 --> 00:03:46,590
It's not ha, as, not exactly the same

78
00:03:46,590 --> 00:03:48,910
as the original data set, but it's pretty
close.

79
00:03:49,950 --> 00:03:52,540
If you move on to the next picture, which
is

80
00:03:52,540 --> 00:03:54,570
letter C, here, you can see that it's a
little bit,

81
00:03:54,570 --> 00:03:56,150
kind of has a little bit more definition.

82
00:03:56,150 --> 00:03:59,120
This is using the first ten pixel,
singular vectors.

83
00:03:59,120 --> 00:04:04,300
And, but it's not very different from the
second one, which uses, only used five.

84
00:04:04,300 --> 00:04:07,000
And then the very last one here on the
right is the original data set.

85
00:04:07,000 --> 00:04:09,130
So, you can see that if you go, if you use

86
00:04:09,130 --> 00:04:12,080
just a few, a singular vector, maybe up to
five or ten.

87
00:04:12,080 --> 00:04:16,040
You can get a reasonable approximation of
this face without

88
00:04:16,040 --> 00:04:19,050
having to kind of store all of the
original data.

89
00:04:19,050 --> 00:04:19,970
So, this is an example

90
00:04:19,970 --> 00:04:22,954
of a kind of data compression type of
approach.

91
00:04:22,954 --> 00:04:27,520
that, that, the singular value
decomposition can can generate.

92
00:04:27,520 --> 00:04:30,320
Now data compression and kind of
statistical summaries

93
00:04:30,320 --> 00:04:32,430
are kind of two sides of the same coin.

94
00:04:32,430 --> 00:04:35,480
And so if you want to summarize the data
set with the, with a,

95
00:04:35,480 --> 00:04:37,860
with a smaller number of features the

96
00:04:37,860 --> 00:04:40,700
singular value decomposition is also
useful for that.

97
00:04:44,150 --> 00:04:46,170
So just a couple of notes and further
resources

98
00:04:46,170 --> 00:04:49,710
for the singular value decomposition and
principle component analysis.

99
00:04:49,710 --> 00:04:52,740
One of the issues is that the scale of
your data matters.

100
00:04:52,740 --> 00:04:54,860
So if you have, for example, it's common
to

101
00:04:54,860 --> 00:04:58,700
measure lot's of different variables that
come on different scales.

102
00:04:58,700 --> 00:05:01,300
And that can cause a problem, because if
one

103
00:05:01,300 --> 00:05:04,540
variable is much larger than another
variable, just because

104
00:05:04,540 --> 00:05:07,070
the unit's so different that will tend to
drive

105
00:05:07,070 --> 00:05:10,080
this, the principle components analysis
over the singular vectors.

106
00:05:10,080 --> 00:05:13,140
And so that may not be particularly
meaningful to you.

107
00:05:13,140 --> 00:05:16,100
And so you want to look at the, see that
the, kind of the

108
00:05:16,100 --> 00:05:19,740
scale of the different columns or rows are
roughly comparable to each other.

109
00:05:21,920 --> 00:05:24,210
the, as we saw in the, one of the, in the
example with the

110
00:05:24,210 --> 00:05:26,020
two different patterns, the principle
components and

111
00:05:26,020 --> 00:05:29,170
the singular vectors may mix together real
patterns.

112
00:05:29,170 --> 00:05:32,070
And so, the patterns that you see may not
represent the kind of

113
00:05:32,070 --> 00:05:33,350
separable patterns but they may be

114
00:05:33,350 --> 00:05:36,020
patterns that may, that are mixed
together.

115
00:05:36,020 --> 00:05:39,600
The singular value decomposition can be
computationally intensive if you have a

116
00:05:39,600 --> 00:05:42,890
very large matrix so that's one, that's
something to keep in mind.

117
00:05:42,890 --> 00:05:45,000
We use relatively small matrices here, but

118
00:05:45,000 --> 00:05:46,930
of course computing power is getting ever
more

119
00:05:48,210 --> 00:05:51,880
powerful and and there there are some
highly optimized and

120
00:05:51,880 --> 00:05:53,680
specialized matrix libraries out there

121
00:05:53,680 --> 00:05:55,940
for computing the singular value
decomposition.

122
00:05:55,940 --> 00:05:58,810
And so this can be done on, on lots of
kind of practical problems.

123
00:06:00,150 --> 00:06:03,720
Without too much kind of planning in
advance.

124
00:06:04,790 --> 00:06:07,780
And so here are a couple links to kind of
further resources

125
00:06:07,780 --> 00:06:09,310
for what, how to use principle

126
00:06:09,310 --> 00:06:11,590
components analysis in the singular value
composition.

127
00:06:11,590 --> 00:06:13,370
And, and also there are other kind

128
00:06:13,370 --> 00:06:15,860
of approaches that are similar to this.

129
00:06:15,860 --> 00:06:19,560
But are a kind of different in many of the
details.

130
00:06:19,560 --> 00:06:20,770
You may hear about these approaches.

131
00:06:20,770 --> 00:06:22,480
Things like factor analysis, independent

132
00:06:22,480 --> 00:06:25,400
components analysis and latent semantic
analysis.

133
00:06:25,400 --> 00:06:28,940
And these are worth exploring, but are
related to the, kind

134
00:06:28,940 --> 00:06:31,070
of the basic ideas behind principle

135
00:06:31,070 --> 00:06:33,620
components analysis and singular valued
decomposition.

136
00:06:33,620 --> 00:06:34,790
Which is that you want to find the kind

137
00:06:34,790 --> 00:06:39,180
of lower dimensional representation that
explains most of the variation

138
00:06:39,180 --> 00:06:40,250
in the data that you see.

